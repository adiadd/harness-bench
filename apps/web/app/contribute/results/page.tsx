import { Badge } from "@workspace/ui/components/badge";
import {
  Card,
  CardHeader,
  CardTitle,
  CardDescription,
  CardContent,
} from "@workspace/ui/components/card";
import { Separator } from "@workspace/ui/components/separator";
import {
  Terminal,
  FileJs,
  GitPullRequest,
} from "@phosphor-icons/react/dist/ssr";

export default function SubmitResultsPage() {
  return (
    <main className="mx-auto max-w-6xl px-6 py-10 space-y-10">
      <div>
        <h1 className="text-3xl font-bold tracking-tight">
          Submit Benchmark Results
        </h1>
        <p className="text-muted-foreground mt-2 max-w-2xl text-lg">
          Run benchmarks locally with the harness-bench CLI and submit your
          results to help the community build a comprehensive performance
          dataset.
        </p>
      </div>

      <Separator />

      {/* Running Benchmarks */}
      <Card>
        <CardHeader>
          <div className="flex items-center gap-2">
            <Terminal className="text-muted-foreground size-5" />
            <CardTitle>Running Benchmarks</CardTitle>
          </div>
          <CardDescription>
            Use the CLI to run tasks against any supported harness
          </CardDescription>
        </CardHeader>
        <CardContent className="space-y-4">
          <p className="text-sm leading-relaxed">
            Install the CLI globally, then run a benchmark suite against a
            specific harness and model combination:
          </p>
          <pre className="bg-muted overflow-x-auto rounded-lg border p-4 text-xs leading-relaxed">
            <code>{`# Install the CLI
bun add -g harness-bench

# Run a specific suite against a harness
harness-bench run \\
  --harness claude-code \\
  --model opus-4-6 \\
  --tasks typescript-challenges

# Run a single task
harness-bench run \\
  --harness claude-code \\
  --model opus-4-6 \\
  --task ts-array-dedup

# Run with multiple repetitions for consistency data
harness-bench run \\
  --harness cursor \\
  --model opus-4-6 \\
  --tasks typescript-challenges \\
  --runs 3`}</code>
          </pre>
          <p className="text-muted-foreground text-sm">
            The CLI will set up isolated environments, dispatch tasks to the
            harness, collect results, and write a JSON output file to{" "}
            <code className="bg-muted rounded px-1.5 py-0.5 text-xs">
              ./results/
            </code>{" "}
            in your current directory.
          </p>
        </CardContent>
      </Card>

      {/* Result Format */}
      <Card>
        <CardHeader>
          <div className="flex items-center gap-2">
            <FileJs className="text-muted-foreground size-5" />
            <CardTitle>Result JSON Format</CardTitle>
          </div>
          <CardDescription>
            Structure of the output generated by the CLI
          </CardDescription>
        </CardHeader>
        <CardContent className="space-y-4">
          <p className="text-sm leading-relaxed">
            Each run produces a JSON file containing metadata, per-task results,
            and aggregate statistics:
          </p>
          <pre className="bg-muted overflow-x-auto rounded-lg border p-4 text-xs leading-relaxed">
            <code>{`{
  "version": "1.0",
  "harness": "claude-code",
  "model": "opus-4-6",
  "suite": "typescript-challenges",
  "timestamp": "2026-02-09T12:00:00Z",
  "environment": {
    "os": "darwin",
    "arch": "arm64",
    "runtime": "bun@1.3.8"
  },
  "results": [
    {
      "taskId": "ts-array-dedup",
      "score": 1.0,
      "passed": true,
      "metrics": {
        "tokens": 4521,
        "cost": 0.037,
        "duration": 12.4,
        "toolCalls": 6,
        "turns": 3
      },
      "validation": {
        "type": "test-suite",
        "testsTotal": 8,
        "testsPassed": 8
      }
    }
  ],
  "aggregate": {
    "totalTasks": 15,
    "passRate": 0.867,
    "avgScore": 0.912,
    "avgCost": 0.042,
    "totalCost": 0.63,
    "avgDuration": 18.3
  }
}`}</code>
          </pre>
          <div className="flex flex-wrap gap-2 pt-1">
            <Badge variant="outline" className="text-xs">
              version: schema version
            </Badge>
            <Badge variant="outline" className="text-xs">
              environment: auto-detected
            </Badge>
            <Badge variant="outline" className="text-xs">
              aggregate: auto-computed
            </Badge>
          </div>
        </CardContent>
      </Card>

      {/* Submission Process */}
      <Card>
        <CardHeader>
          <div className="flex items-center gap-2">
            <GitPullRequest className="text-muted-foreground size-5" />
            <CardTitle>Submission Process</CardTitle>
          </div>
          <CardDescription>
            How to submit your results to the public dataset
          </CardDescription>
        </CardHeader>
        <CardContent>
          <ol className="list-decimal space-y-3 pl-5 text-sm leading-relaxed">
            <li>
              Run the benchmark suite using the CLI. Ensure the run completes
              without errors and the output JSON is valid.
            </li>
            <li>
              Fork the{" "}
              <code className="bg-muted rounded px-1.5 py-0.5 text-xs">
                harness-bench
              </code>{" "}
              repository and create a branch named{" "}
              <code className="bg-muted rounded px-1.5 py-0.5 text-xs">
                results/&lt;harness&gt;-&lt;model&gt;-&lt;date&gt;
              </code>
              .
            </li>
            <li>
              Copy your result JSON file into{" "}
              <code className="bg-muted rounded px-1.5 py-0.5 text-xs">
                data/results/&lt;harness&gt;/&lt;model&gt;/
              </code>{" "}
              in the repository.
            </li>
            <li>
              Run{" "}
              <code className="bg-muted rounded px-1.5 py-0.5 text-xs">
                harness-bench validate --results &lt;path-to-json&gt;
              </code>{" "}
              to verify the file passes schema validation and integrity checks.
            </li>
            <li>
              Open a pull request. Automated CI will re-validate the JSON, check
              for duplicate submissions, and verify that the referenced tasks
              exist in the task registry.
            </li>
            <li>
              Once merged, your results will appear on the dashboard within a
              few minutes. The leaderboard and comparison views update
              automatically.
            </li>
          </ol>
        </CardContent>
      </Card>
    </main>
  );
}
